---
title: "Approximating logistic coefficients"
subtitle: 'A simulation analysis'
author: "Developed by [Gabriel Hoffman](http://gabrielhoffman.github.io/)"
date: "Run on `r Sys.time()`"
documentclass: article
output: 
  html_document:
      fig_caption: yes
  toc: true
  smart: true
vignette: >
  %\VignetteIndexEntry{Approximating logistic coefficients}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
  %\usepackage[utf8]{inputenc}
---


```{r setup, echo=FALSE, results="hide"}
knitr::opts_chunk$set(
  tidy = FALSE,
  cache = TRUE,
  echo = FALSE,
  dev = c("png", "pdf"),
  package.startup.message = FALSE,
  message = FALSE,
  error = FALSE,
  warning = FALSE
)
``` 

Given a z-statistic, we want to obtain the coefficient value from a linear regression. Adapting the approach from [Zhu, et al (2016, Methods eqn 6)](https://doi.org/10.1038/ng.3538), we estimate the coefficient as $$\beta_{linear} = z * sd_y / (sd_x*sqrt(n + z^2)),$$ where \(sd_x\) is the standard deviation of the covariate and \(sd_y\) is the standard error of the response. For a model with no covariates, this transformation gives the exact coefficient estimate. With covariates, it is approximate.

The coeffient estimate from linear regression can be converted to the logistic scale using the first order approach of [Pirinen, et al. (2013)](https://doi.org/10.1214/12-AOAS586) according to $$\beta_{logistic} = \beta_{linear} / (\phi(1-\phi)),$$ where \(\phi\) is the case ratio in the logistic regression. This approximates the coefficient as if the model had been fit with logistic regression.  This is implemented in `coef_from_z()`.  We also implement a second order approach, and an independent method described by [Lloyd-Jones, et al. (2018)](https://doi.org/10.1534/genetics.117.300360).

We see here that using the z-statistic to approximate the coefficient from logistic regression model is most accurate for large sample size, balanced class ratio (i.e. $\phi$ near 0.5), and small $\beta$. 

```{r load, cache=FALSE}
library(imputez)
library(tidyverse)
library(ggplot2)
library(cowplot)

# simulate data
get_estimates = function(mu, beta, n, p, ...){

  x = rbinom(n, 2, p)
  x = x - mean(x)

  eta = mu + x*beta
  y = rbinom(n, size=1, prob=plogis(eta))
  data = data.frame(x, y)

  fit <- glm(y ~ x, data=data, family=binomial)
  z = coef(summary(fit))[2,3]
  phi = sum(y) / length(y)

  beta_logit = coef(summary(fit))[2,]

  beta_hat = coef_from_z(z, n, sd(x), phi=phi, p=p,...)

  data.frame(beta_logit = beta_logit[1], 
            beta_hat = beta_hat[1],
            phi = phi)
}
```

```{r sim1}
p = .5

# Sample size
#############

n_array = 10^seq(2.2, 5, length.out=20)

res = lapply(floor(n_array), function(n){
  lapply(seq(100), function(i){
    set.seed(i)
    get_estimates(0, .1, n, p)
  }) %>%
  bind_rows %>%
  summarize(rMSE = sqrt(mean((beta_logit-coef)^2))) %>%
  mutate(n = n)
})
res = bind_rows(res)

thm = theme_classic() +
    theme(aspect.ratio=1)

fig1 = res %>%
  ggplot(aes(n, rMSE )) +
    geom_point() +
    thm +
    scale_x_log10() +
    scale_y_log10() +
    ylab("Root mean squared error")
```


```{r sim2}
# Phi
######

n = 10000
mu_array = seq(-2.5, 2.5, length.out=30)

res = lapply(mu_array, function(mu){
  lapply(seq(100), function(i){
    set.seed(i)
    get_estimates(mu, 0, n, p)
  }) %>%
  bind_rows %>%
  summarize(rMSE = sqrt(mean((beta_logit-coef)^2)), phi = mean(phi)) %>%
  mutate(mu = mu)
})
res = bind_rows(res)

fig2 = res %>%
  ggplot(aes(phi, rMSE )) +
    geom_point() +
    thm +
    scale_x_continuous(limits=c(0, 1), expand=c(0,0)) +
    xlab(bquote(phi)) +
    ylab("Root mean squared error")
```


```{r sim3}
# beta
######

n = 10000
mu = -3
beta_array = seq(-2, 2, length.out=30)

res = lapply(beta_array, function(beta){
  lapply(seq(10), function(i){
    set.seed(i)
    get_estimates(mu, beta, n, p)
  }) %>%
  bind_rows %>%
  summarize(rMSE = sqrt(mean((beta_logit-coef)^2))) %>%
  mutate(beta = beta)
})
res = bind_rows(res)

fig3 = res %>%
  ggplot(aes(beta, rMSE, color )) +
    geom_point() +
    thm +
    xlab(bquote(beta)) +
    ylab("Root mean squared error")
```


```{r plot, cache=FALSE, fig.width=8, fig.height=2.8, fig.cap="**Figure 1. Accuracy of approximating $\\beta$ from z-statistics.** **A**) Sample size is varied with $\\beta=0$, $\\phi=0.5$ **B**) Case ratio is varied with $\\beta=0$, $n=10,000$ **C**) $\\beta$ is varied with $\\phi=0.5$, $n=10,000$"}
plot_grid(fig1, fig2, fig3, nrow=1, labels=LETTERS[1:3])
```


```{r compare}
run_sims = function(n, mu, p){

  beta_array = seq(-2, 2, length.out=30)

  res1 = lapply(beta_array, function(beta){
    lapply(seq(10), function(i){
      set.seed(i)
      get_estimates(mu, beta, n, p, useMethod=1)
    }) %>%
    bind_rows %>%
    mutate(beta = beta, Method = "1")
  }) %>%
    bind_rows

  res2 = lapply(beta_array, function(beta){
    lapply(seq(10), function(i){
      set.seed(i)
      get_estimates(mu, beta, n, p, useMethod=2)
    }) %>%
    bind_rows %>%
    mutate(beta = beta, Method = "2")
  }) %>%
    bind_rows

  res3 = lapply(beta_array, function(beta){
    lapply(seq(10), function(i){
      set.seed(i)
      get_estimates(mu, beta, n, p, useMethod=3)
    }) %>%
    bind_rows %>%
    mutate(beta = beta, Method = "3")
  }) %>%
    bind_rows

  bind_rows(res1, res2, res3) %>%
    mutate(mu = mu, p = p)
}

res = bind_rows(
  run_sims(10000, 0, 0.5),
  run_sims(10000, 1, 0.5),
  run_sims(10000, 2, 0.5),
  run_sims(10000, 0, 0.2),
  run_sims(10000, 1, 0.2),
  run_sims(10000, 2, 0.2),
  run_sims(10000, 0, 0.05),
  run_sims(10000, 1, 0.05),
  run_sims(10000, 2, 0.05))
```

```{r plot2, cache=FALSE, fig.width=8, fig.height=7, fig.cap="**Figure 2. Accuracy of approximating $\\beta$ from z-statistics across a range of MAF and $\\phi$ values.** Simulation were performed with $n=10,000$, while varying the MAF along rows and $\\phi$ along columns.  Estimates were generated with 3 methods."}
res %>%
  mutate(Method = factor(Method)) %>%
  mutate(Method = fct_recode(Method, 
                      "First order" = "1", 
                      "Second order" = "2", 
                      "Lloyd-Jones" = "3")) %>% 
  group_by(Method, beta, mu, p) %>%
  summarize(beta_logit = mean(beta_logit), coef = mean(coef)) %>%
  ggplot(aes(beta_logit, coef, color=Method, shape=Method)) +
  geom_abline(slope=1) +
  geom_vline(xintercept=0, color="grey80") +
  geom_hline(yintercept=0, color="grey80") +
  geom_point() +
  theme_classic() +
  theme(aspect.ratio=1) +
  xlab(bquote(true~beta)) +
  ylab(bquote(Estimated~beta)) +
  facet_grid(p ~ round(plogis(mu), 2))
```


# Session Info
<details>
```{r session, echo=FALSE}
sessionInfo()
```
</details>





